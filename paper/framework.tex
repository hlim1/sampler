\section{Sampling Framework}
\label{sec:framework}

This section describes our sampling framework.  We begin with sampling
of basic blocks and gradually add features until we can describe how
to perform sampling for entire programs.  Suppose we start with the
following C code:

\begin{code}
  check(p !=\ NULL); \\
  \up p = p->next; \\
  \\
  check(i < max); \\
  \up total += sizes[i];
\end{code}

Our sampling framework can be configured to sample arbitrary pieces of
code, which may be either portions of the original program or
instrumentation predicates added separately.  For this particular
example, assume that the italicized \texttt{\textit{check()}} calls
have been tagged for sampling.  A \texttt{\textit{check()}} call might
conditionally halt the program (as with \texttt{assert()}), or it
might append an event to a trace stream, or it might update a
per-predicate counter to record how often the predicate is true.  The
precise behavior of the instrumentation code is of no concern to the
sampling transformation itself.

\subsection{Sampling the Bernoulli Way}

Suppose that we wish to sample one hundredth of these checks.
Maintaining a global counter modulo one hundred is simple, but has the
disadvantage of being trivially periodic.  If the above fragment were
in a loop, for example, one of the checks would execute on every
fiftieth iteration while the other would never execute.  We wish
sampling to be fair and uniformly random: each check should
independently have a \nicefrac{1}{100} chance of being sampled each
time it occurs.  This is a so-called \termdef{Bernoulli process}, the
most common example of which is repeatedly tossing a coin.  We wish to
sample based on the outcome of tossing a coin which is biased to come
up heads only one time in a hundred.

A \naive approach would be to use a simple random number generator.
Suppose \texttt{rnd($n$)} yields a random integer uniformly
distributed between 0 and $n-1$.  Then the following code gives us
fair random sampling at the desired density:

\begin{code}
  if(rnd(100) == 0) check(p != NULL); \\
  \up p = p->next; \\
  \\
  if(rnd(100) == 0) check(i < max); \\
  \up total += sizes[i];
\end{code}

This strategy has some practical problems.  Random number generation
is not free: tossing the coin may be slower than simply doing the
check unconditionally.  Furthermore, what was previously straight-line
code is now dense with branches and joins, which may impede other
optimizations.

Sampling is sparse.  Each of the conditionals has a \nicefrac{99}{100}
= 99\% chance of not sampling.  On any run through this block, there
is a $(\nicefrac{99}{100})^2 \approx 98\%$ chance that both
instrumentation sites are skipped.  If we determine, upon reaching the
top of a basic block, that no site in that block is sampled, then we
can branch into fast-path code with all conditionally-guarded checks
removed.  This requires two versions of the code: one with sampled
instrumentation, one without.  It also requires that we can predict
how many future sampling opportunities are skipped before the next one
is taken.

Anticipating future samples requires a change in randomization
strategy.  Consider a sequence of biased coin tosses, with ``0''
indicating no sample and ``1'' indicating that a sample is to be
taken.  Temporarily increasing the sampling density to
\nicefrac{1}{5}, we might have:
%%
\begin{equation*}
  \langle
  \underbrace{0, 0, 0, 0, 0, 1}_6,
  \underbrace{0, 0, 0, 1}_4,
  \underbrace{0, 1}_2,
  \underbrace{0, 0, 1}_3,
  \dots
  \rangle
\end{equation*}

An equivalent representation counts the number of tosses until (and
including) the next sampled check: $\langle6, 4, 2, 3, \dots \rangle$.
This representation is predictive: the head of the sequence can be
treated as a countdown, telling us how far away the next sample is.
If we are at the top of a basic block containing only two checks, and
the next sampling countdown is 6, we know in advance that neither of
those sites is sampled on this visit.  Instead, we merely discard two
tosses and proceed directly to the instrumentation-free fast path:

\begin{code}
  if(countdown > 2) \{ \\
  \> /* fast path: no sample ahead */ \\
  \> countdown -= 2; \\
  \> \up p = p->next; \\
  \> \up total += sizes[i]; \\
  \} else \{ \\
  \> /* slow path: sample is imminent */ \\
  \> if(countdown-- == 0) \{ \\
  \>\> check(p != NULL); \\
  \>\> countdown = getNextCountdown(); \\
  \> \} \\
  \> \up p = p->next; \\
  \> \\
  \> if(countdown-- == 0) \{ \\
  \>\> check(i < max); \\
  \>\> countdown = getNextCountdown(); \\
  \> \} \\
  \> \up total += sizes[i]; \\
  \}
\end{code}

The instrumented code does extra work to manage the
next-sample countdown, but the fast path is much improved.  The only
overhead is a single compare/branch and a constant decrement, and this
overhead is amortized over the entire block.  In larger blocks with
more instrumentation sites, the initial countdown check has a larger
threshold, but that one check suffices to predict a larger number of
skipped sampling opportunities.

Consider the distribution of countdown values.  With a sampling
density of \nicefrac{1}{100}, there is a \nicefrac{1}{100} chance that
we sample at the very next opportunity.  There is a
$(\nicefrac{99}{100}) \times (\nicefrac{1}{100})$ that the next chance
is skipped but that the one after that is taken.  A countdown of three
appears on a $(\nicefrac{99}{100})^2 \times (\nicefrac{1}{100})$
chance, and so on.  These numbers form a \termdef{geometric
  distribution} whose mean value is the inverse of the sampling
density (that is, 100).  Numbers in a geometric distribution
characterize the expected inter-arrival times of a Bernoulli process.
However, repeated tossing of a biased coin is not necessary:
geometrically distributed random numbers can be generated directly
using a standard uniform random generator and some simple
floating-point operations.\footnote{In theory, a countdown may need to
  be arbitrarily large.  However, the odds of a \nicefrac{1}{100}
  countdown exceeding $2^{32}-1$ are less than one in $10^{10^7}$.}

As can be seen in the instrumented slow path, the countdown is reset
once it reaches zero.  Thus, we consume next-sample countdowns
gradually over time.  However, the rate of consumption is slower than
that for raw coin tosses: $n$ countdowns for $\nicefrac{1}{d}$
sampling encode, on average, $nd$ tosses.  A bank of pre-generated
random countdowns, then, is quite reasonable and will exhaust or
repeat $d$ times more slowly than would a similar bank of raw coin
tosses.

\subsection{From Blocks to Functions}

The scheme for blocks outlined above generalizes to an arbitrary
control flow graph as follows.  Any region of acyclic code has a
finite number of possible paths.  Let the maximum number of
instrumented sites on any path be the \termdef{weight} of that region.
If the next-sample countdown exceeds the weight of an acyclic region,
then no samples are taken on this pass through that part of the
code.  We can place a countdown threshold check at the top of this
region.

Cycles without instrumentation are effectively weightless and may be
disregarded.  A cycle with instrumentation must also contain a
threshold check.  This guarantees that if we start at any threshold
check and execute forward, we cross only a finite number of
instrumentation sites before reaching the next threshold check.  Thus,
we can compute a finite weight for each threshold check.

There is some flexibility regarding exactly where a threshold check is
placed; an optimal solution here is NP-hard
\cite{Hirzel:2001:BT-FLOTP}.  For simplicity, our present system
places one threshold check at function entry and one along each loop
back edge.  Weights may be computed in a single bottom-up traversal of
each function's control flow graph.  If any threshold check is found
to have zero weight, it is simply discarded.

We produce two complete copies of the function body.  One contains
full instrumentation, with each possible sample guarded by a decrement
and test of the next-sample countdown.  The other copy, the fast path,
merely decrements the countdown where appropriate, but otherwise has
all instrumentation removed.  We stitch the two variants together at
threshold check points: at the top of each acyclic region, we decide
whether a sample is imminent.  If it is, we branch into the
instrumented code.  If the next sample is far off, we continue in the
fast path code instead.

\begin{figure}
  \centering
  \input{layout}
  \caption{Example of instrumented code layout}
  \label{fig:code-layout}
\end{figure}

Figure~\ref{fig:code-layout} shows an example of code layout for a
function containing one conditional and one loop.  Dotted nodes
represent instrumentation sites; these are reduced to countdown
decrements in the fast path.  The boxed nodes represent threshold
checks; we have added one at function entry and one along the back
edge of the loop.  This code layout strategy is a variation on that
used by Arnold and Ryder to reduce the cost of code instrumented for
performance profiling \cite{Arnold:2001:FRC}.  The principal change in
our transformation is the use of geometrically distributed countdowns
in conjunction with acyclic region weights to choose between the two
code variants.  Arnold and Ryder use fixed sampling periods (e.g.,
exactly once per $n$ opportunities, or exactly once per $n$
instructions) and do not apply region-specific weighting.  Our
approach imposes more overhead, but offers greater statistical rigor
in the resultant sampled data.  Arnold and Ryder have studied several
variant layouts with varying trade-offs of code size versus overhead;
the same choices and trade-offs are directly applicable here.

\subsection{Function Calls}
\label{sec:framework:calls}

New optimization opportunities arise in the presence of function
calls.  A conservative treatment assumes any function call changes the
countdown arbitrarily.  Therefore, a new threshold check must appear
immediately after each function call.  This treatment is appropriate
if, e.g., the callee is being compiled separately.

However, if the callee is known and available for examination, a
simple interprocedural analysis can be used.  A \termdef{weightless
  function} has the following properties:

\begin{itemize}
\item The function contains no instrumentation sites.
\item The function only calls other weightless functions.
\end{itemize}

The set of weightless functions can be computed iteratively, requiring
no more steps in the worst case then the depth of the deepest
non-recursive call chain.

For purposes of identifying acyclic regions and placing threshold
checks, calls to weightless functions are invisible.  Acyclic regions
can extend below such calls, and no additional threshold check is
required after such a call returns.  A further benefit is that the
bodies of weightless functions may be compiled with no modifications.
They have no threshold checks, no instrumented code, and therefore
require no cloning or transformation of any kind.

\subsection{Global Countdown Management}

Our initial experience suggests that having the next-site countdown in
a global variable can be expensive.  Our system is implemented as a
source-to-source transformation for C, with \texttt{gcc} as our native
compiler.  We find that \texttt{gcc} treats the many
``\texttt{countdown--}'' decrements along the fast path quite poorly.
It will not, for example, coalesce a sequence of five such decrements
into a single ``\texttt{countdown -= 5}'' adjustment.  This apparently
stems from conservative assumptions about aliasing of global
variables.

Efficient countdown management requires that the native C compiler
take greater liberties when optimizing these decrements.  We assist
the native compiler by maintaining the countdown in a local variable
within each function:

\begin{enumerate}
\item At function entry, \termdef{import} the current global countdown
  into a local variable.
\item Use this local copy for all decrements, threshold checks, and
  sampling decisions.
\item Just before function exit, \termdef{export} this local copy back
  out to the global.
\end{enumerate}

To maintain agreement across all functions, we must also
export just before each function call and import again after each call
returns.  Again, though, calls to weightless functions may simply be
ignored, as they do not change or even inspect the countdown.
Similarly, the bodies of weightless functions need not import and
export at entry and exit, since they always leave the countdown
unchanged.  With this change, the conventional native C compiler can
coalesce decrements and perform other standard but important
optimizations.

\subsection{Issues in Remote Sampling}
\label{sec:compression}

Our framework for statistically fair sampling can be used for any
program monitoring application.  As discussed in
Section~\ref{sec:introduction}, there are issues peculiar to
monitoring a large number of remote sites.  Here we briefly discuss
the main problems and a particular solution that we adopt.

There are several dimensions in which performance can be harmed by
remote monitoring.  As usual the performance penalty imposed by the
extra monitoring code is a concern, but so are the use of local
storage to hold results (even temporarily) on a user's machine, the
use of network bandwidth to transmit results, and finally the storage
needed to hold results on a central server for analysis.  For example,
if we wish to retain all sampled data, then the storage requirements for the
central server grow linearly with the number of executions even if the
data collected from each execution is constant size.  
%In
%Section~\ref{sec:ccured} we give an example of an analysis that does
%not require retention of the data on the server.  However, we expect
%it will often be desirable to be able to perform a new analysis on
%previously gathered data and in this situation at least some data
%would need to be retained.

Our approach is to sample the number of observations of each of a very
large, but fixed, set of predicates (see Sections~\ref{sec:ccrypt}
and~\ref{sec:bc}).  The final form of the data is a vector of
integers, with position $i$ containing the number of times we observed
that the $i$th predicate was true.  For example, a typical entry might
be that the predicate $x > y$ at a particular program point was
observed to be true 42 times in one execution.

Maintaining a vector of counters produces data for an execution whose
size is largely independent of the sampling density or running time.
The loss of information, however, is significant, as the order
of the observations is discarded.  While our results can be interpreted
as showing that one can go a long way ignoring ordering, we expect
there are interesting applications that require ordering information.
We leave the problem of determining how to efficiently gather, store and
analyze partial traces (with ordering information) as future work.

%% LocalWords: rnd getNextCountdown pre
