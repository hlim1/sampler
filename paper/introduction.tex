\section{Introduction}
\label{sec:introduction}

It is an unfortunate fact that essentially all deployed software
systems have bugs, and that users often encounter these bugs.  The
resources (measured in time, money, or people) available for improving
software are always limited, and the normal case is that just through
sheer numbers the user community brings far more resources to bear on
testing a piece of software than the team responsible for producing
that software.

This paper is about making lemonade from lemons.  Given that deployed
software has problems, perhaps we can speed up the process of
identifying and eliminating those problems by learning something from
the enormous number of executions performed by the software's user
community.  We propose an infrastructure where some information about
each user execution of a program is transmitted to a central database.
The data gathered from all executions is analyzed to extract
information that helps engineers find and fix problems more quickly;
we call this {\em automatic bug isolation}.
In our view, such an infrastructure has several benefits:
\begin{itemize}
  
\item For deployed software systems, the number of executions in
  actual use dwarfs the number of executions produced in testing by
  orders of magnitude.  For many software systems today, essentially
  all of the information from actual executions is discarded, because
  there is no mechanism for feedback.  Retaining even a small portion
  of that information could be valuable.
  
\item Gathering information from all, or at least a representative
  sample, of user executions gives an accurate picture of how the
  software is actually used, allowing better decisions about how to
  spend scarce resources on modifications. In particular, bugs that
  affect a large number of users are a higher priority than bugs that
  are very rare.  This kind of information is almost impossible to
  obtain from anywhere other than actual user executions.
  
\item While our primary interest is in finding and fixing quality
  problems, information gathered from user executions could be useful
  for other purposes.  For example, software authors may simply wish
  to know which features are most commonly used, or we may be
  interested in discovering whether code not covered by in-house
  testing is ever executed in practice, etc.
  
\item Traditional user feedback about problems often involves a call
  from a relatively unsophisticated user to a perhaps only somewhat
  more sophisticated technical support center.  In a networked world,
  it is simply more efficient and accurate to gather this information
  automatically.
  
\item Many bugs sit on open bug lists of products for an extended
  period of time before an engineer is available to work on the bug.
  Automatically gathering data from user executions allows for
  automated analysis without human intervention.  Thus, when an
  engineer is finally available to work on a problem, the results of
  automated analyses done in the interim may help the engineer to
  identify and fix even relatively simple problems more quickly.
  
  \disregard{
  \item Based on the current state of automatic analysis, clients can
    be instructed to focus attention in data gathering on different
    aspects of the software.  For example, if there appears to be a
    problem with a particular function $f$, we may wish clients to
    disproportionately report information about $f$ for a period of
    time, to speed up the process of gathering enough data for
    analysis.}

\end{itemize}

The idea of gathering data from actual user executions is not new.
Commercial databases, for example, routinely produce extensive log
files, and the first action when a user reports a problem is to
inspect those logs.  Similarly, each flight of the Boeing 777
generates logs which are subsequently combed for signs of possible
problems \cite{Esler:2001:WVR}.  There are many other similar examples
in the world of commercial software.

A more recent development is the result of ubiquitous Internet
connectivity.  Netscape/Mozilla, Microsoft, GNOME, and KDE have all
deployed automated, opt-in crash reporting systems.  These systems
gather key information about program state after a failure has
occurred: stack trace, register contents, and the like.  By sending
this information back to the development organization, the community
helps developers effectively triage bugs that cause crashes and focus
on the problems experienced by the most users.

We believe this is progress in the right direction, but we also
believe that existing approaches only scratch the surface of what is
possible when developers and users are connected by a network.  For
example, the crash-reporting systems do not gather any information
about what happened before the crash.  Trace information leading up to
the failure may contain critical clues to the actual problem.  Also,
crash reporting systems report no information for successful runs,
which makes it difficult to distinguish anomalous (crash-causing)
behavior from innocuous behavior common to all runs.  In general, the
information gathered by crash-reporting systems is not very
systematic, and in all feedback systems of which we are aware
(crash-reporting or otherwise) the subsequent data analysis is highly
manual.

We present one approach to systematically gathering information about
program runs from a large, distributed user community and performing
subsequent automatic analysis of that information to help in isolating
bugs.  Initially, we believed that the interesting problem was the
analysis of the data, and that gathering the data was relatively
straightforward.  However, we discovered that designing a data
gathering infrastructure that would scale is non-trivial.  As a
result, this paper is as much about the design and implementation of
the system that gathers the data from user executions
(Section~\ref{sec:framework}) as it is about the subsequent data
analysis (Section~\ref{sec:applications}).

Our infrastructure is designed to gather information about a large
number of program executions taking place remotely from a central site
where data is collected.  Any such design must solve two critical
problems.

The first problem is that the method for gathering information must be
sufficiently lightweight that it has a negligible impact on the
performance of the user's program.  Our approach, discussed in
Section~\ref{sec:framework}, is based on sampling.  Classical sampling
for measuring program performance searches for the ``elephant in the
haystack'': it is looking for the biggest consumers of time.  In
contrast, we are looking for needles (bugs) that may occur very
rarely, and furthermore our sampling rates may be very low to maintain
client performance.  This leads us to be concerned with guaranteeing
that the sampling is statistically fair, so that we can rely on the
reported frequencies of rare events.  We also develop new ways to
reduce the overhead of the necessary additional code that determines
whether to take a sample or not.

The second problem is that information from the client must be
transmitted over the network to the central database.  Gathering even
a relatively small amount of data periodically from a large number of
clients creates significant scalability problems.  We have found it necessary
to discard information about the order in which program statements execute
to achieve sufficiently compact representations of sampled data
(see Section~\ref{sec:compression}).

Section~\ref{sec:applications} presents several different applications
of our framework.  In Section~\ref{sec:ccured} we consider how we can
share the cost of program assertions over a large user base through
sampling.  Each user only executes a fraction of the assertions, and
thus sees good performance, but in the aggregate bugs due to assertion
failures are still extremely likely to be detected.  In
Section~\ref{sec:ccrypt} we pursue a bug without the benefit of
explicit assertions.  Instead, an initial set of broad guesses is
whittled down over time as sampled executions reveal which behaviors
do and do not coincide with program failure.  Section~\ref{sec:bc}
further generalizes our approach to deal with non-deterministic
failures.  We sample information on the relationship between pairs of
integer variables in a program from both successful and failing runs
and use statistical regression techniques to identify those predicates
that are highly predictive of program failure.

\aside{A very similar road map appears at the top of
  Section~\ref{sec:applications}.  Are both needed?}

Of course, monitoring of user executions raises some important privacy
and security concerns.  The problems are both social and technical; a
discussion of these issues appears in Section~\ref{sec:privsec}.

%% LocalWords:  privsec
