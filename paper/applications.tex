\section{Applications and Experiments}
\label{sec:applications}

We have used the transformation framework to inject different forms of
instrumentation into several programs, both to assess the performance
impact of sampling as well as to show that sampling can offer novel
insights into program (mis)behavior.

\subsection{Sharing the Cost of Assertions}
\label{sec:share}

In conventional usage, C \texttt{assert()} calls are used during
program development but are disabled (``\texttt{-DNDEBUG}'') when the
code ships to boost performance.  However, deployed programs fail in
unanticipated ways, and it would be helpful to retain some level of
assertion checking if the performance penalty were not excessive.

Programs produced by the \CCured instrumenting translator
\cite{POPL_'02*128} are extreme examples of assertion-dense code.
\CCured analyzes programs written in C and attempts to prove,
statically, that most pointer operations are memory safe.  Where this
cannot be done, \CCured inserts dynamic checks to enforce memory
safety at run time.  For purposes of our research, \CCured is simply a
source of highly assertion-dense code.  The individual assertions are
quite small and fast: array bounds checks, testing for null, etc.
However, in large numbers, their performance impact can be
considerable.  We wish to use random sampling to spread this cost
among many users.

We have applied sampling to several benchmarks taken from the Olden
\cite{Carlisle:1996:OPPWDDSDMM} and SPECINT95 \cite{SPEC95} benchmark
suites.  All programs run to completion and we are simply measuring
the overhead of performing the dynamic checks.

\subsubsection{Whole-Program Sampling}
\label{sec:share:whole}

\begin{table*}[tb]
  \centering
  \small
  \begin{tabular}{|l|rrr|rrr|}
    \hline
    & \multicolumn{3}{c|}{\textbf{function counts}} & \multicolumn{3}{c|}{\textbf{average for functions with sites}} \\
    \raisebox{1.5ex}[0pt]{\textbf{benchmark}} & \textbf{total} & \textbf{weightless} & \textbf{has sites} & \textbf{sites} & \textbf{threshold checks} & \textbf{threshold weight} \\
    \hline\hline
    \input{applications/olden-static}
    \hline
    \input{applications/spec95-static}
    \hline
  \end{tabular}
  \caption{Static metrics for \CCured benchmarks.  Olden benchmarks
    are listed first, followed by SPECINT95.}
  \label{tab:share:static}
\end{table*}

\autoref{tab:share:static} summarizes static aspects of the sampling
transformation when applied to the entirety of each benchmark.  For
each program, we give the total number of non-library functions and
the number of these that are weightless.  \CCured is a whole-program
analysis, so weightless function identification has the advantage of
being able to examine every function body.  We also count the number
of functions which directly contain at least one instrumentation site.
(What remains are those functions which contain no sites of their own
but which call other functions that do have sites.)

Considering just the functions which directly contain at least one
instrumentation site, \autoref{tab:share:static} also presents the
average number of sites per function, the average number of threshold
check points per function, and the average threshold weight for all
such points.  (Note that the product of the last two of these metrics
may exceed the first, as a single instrumentation site may fall under
more than one threshold check point.  This can be seen in the example
in \autoref{fig:code-layout} as well.)  The average site count gives
an impression of how assertion-dense the code is.  The average
threshold weight measures how effective our transformation has been in
amortizing the cost of countdown checks over multiple sites.
Single-site functions are not uncommon; thus, an average threshold
weight above two is encouraging because it suggests that overall
amortization rates are fairly good.

\begin{table}
  \centering
  \begin{tabular}{|l|rrrr|}
    \hline
    \rule{0pt}{2.5ex}
    \textbf{benchmark} & $\mathbf{10^{-2}}$ & $\mathbf{10^{-3}}$ & $\mathbf{10^{-4}}$ & $\mathbf{10^{-6}}$ \\
    \hline\hline
    \input{applications/olden-density}
    \hline
    \input{applications/spec95-density}
    \hline
  \end{tabular}
  \caption{Relative speedup of various sampling rates versus always checking}
  \label{tab:share:density}
\end{table}

\autoref{tab:share:density} shows the performance effect of various
sampling densities.  We take the running time of standard \CCured code
as the baseline.  We report the speedup ($>1$) or slowdown ($<1$)
relative to this baseline when sampling at various densities.  All
benchmarks were compiled using \texttt{gcc} 2.96 using standard
optimization (\texttt{-O2}).  Times were collected on a 550 MHz
Pentium III Linux machine with two gigabytes of RAM.  Reported
speedups represent the average of four runs; each run used a different
pre-generated bank of 1024 geometrically distributed random
countdowns.

Even at a fairly high sampling density of 1/100, more than two thirds
of our benchmarks run faster then they would if all checks were
performed.  Because each single check is so small and fast, this
suggests that we have been successful in amortizing our sampling
overhead.  The most dramatic speedups are seen in \texttt{ijpeg} (24\%
faster) and \texttt{compress} (32\% faster).  On the other hand, three
benchmarks do run slower, with \texttt{go} showing the largest
penalty.  In these cases, the time recovered by skipping 99/100 checks
is not enough to mask the added overhead of sampling.

As we reduce the sampling density to 1/1000, \texttt{power} reaches a
balance point of no measurable speedup or slowdown, while
\texttt{perimeter} remains a stubborn 3\% slower.  The \texttt{health}
benchmark crosses over and is now slightly faster with sampling.
Other benchmarks, which already showed speedups at 1/100, either
retain their position (\texttt{treeadd} and \texttt{tsp}) or show
additional boosts, with \texttt{compress} reaching an overall 38\%
speedup versus standard \CCured.  Further reducing the sampling
density to 1/10,000 shows little change, and by the time we reach
1/1,000,000 it is clear that we have reached a performance ceiling.
One benchmark continues to run slower than standard \CCured; two run
at the same speed; the remaining ten run 2\% - 39\% faster.

\subsubsection{Single-Function Sampling}

In an effort to better understand the sources of overhead, we have
performed additional experiments in which only a single function is
instrumented at a time.  This also approximates a more realistic
approach to managing code size.  The fully instrumented executables
from \autoref{sec:share:whole} range from 2\%-76\% larger than their
non-sampling counterparts.  When only a single function is
instrumented, code expansion is small enough to not be a concern.

A fully deployed system might use dynamic instrumentation to randomly
instrument a single selected function \textit{du jour}.  For the
present research, we build one executable for each site-containing
function as counted in \autoref{tab:share:static}.  All sites in other
functions are removed.  This, in turn, allows us to discover many more
weightless functions: any function which cannot transitively call the
one function being instrumented is weightless.  Thus, a suite of
single-function sampling executables may perform better, over all,
than a single executable with sampling in all functions.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{applications/perimeter}
  \caption{\texttt{perimeter} running times with single-function
    instrumentation}
  \label{fig:share:perimeter}
\end{figure}

In the interest of space, we report only on \texttt{perimeter}: the
stubborn worst performer from the preceding experiment.  This
benchmark's small function count also makes overall results easier to
visualize.  \autoref{fig:share:perimeter} shows absolute running times
in seconds, with 1/100 sampling.  The two leftmost ``all'' bars
represent instrumenting all functions, either with checks always done
or with checks sampled.  The solo ``none'' bar represents the extreme
limit of performance: all instrumentation statically removed from all
functions.  No instrumentation scheme, with or without sampling, can
run faster than this.  The remaining bars show the running time for
each site-containing function, with that function's sites either
checked always or sampled 1/100.

Note that the vertical baseline is nine seconds, not zero, which
magnifies fairly small differences: the two leftmost bars differ by
only 4\%, the same difference reported in the ``$10^{-2}$'' column for
\texttt{perimeter} in \autoref{tab:share:density}.

Among the site-containing functions, \texttt{child()} gains a small
performance boost from sampling.  The remaining five are all
self-recursive, with between one and four recursive calls.  Our
current system treats each of these as a call to a non-weightless
function: the countdown must be exported before the call and imported
after.  A faster approach would be to transform each function to pass
the local countdown down to its recursive callees, and accept an
updated countdown back as a returned value.  Such a countdown
management strategy would be applicable any time both callee and
caller can agree on the API change; the case of self-recursive calls
is particularly straightforward.  It is worth noting that
\texttt{child()}, which does show sampling speedup, is the only
function of these six which is not self-recursive.

Benchmarks that are less dominated by recursion suggest additional
areas for improvement.  Recurring trends include:

\begin{itemize}
\item Low amortization due to lightweight regions in loops with bounds
  that are constant or fixed on entry.  A loop with iteration count
  $i$ and body weight $w$ could be treated as an acyclic region of
  weight $iw$.

\item Conservative identification of weightless functions.  Calls
  through function pointers or to externally-defined code are assumed
  to potentially recurse back to the caller.  Points-to analyses can
  improve the former; a statically checkable \texttt{weightless}
  annotation on function prototypes would resolve the later.
\end{itemize}

When multiple functions are being instrumented, we find another area
for improvement.  Some callees may not be weightless, but can never
cross more than some finite number of sites per call.  The caller of a
finite-weight function can exploit this upper limit, effectively
treating the callee as a cluster of sites of appropriate weight.

We expect that these and other optimizations will continue to improve
the performance benefit from sampling, both for single-function as
well as whole-program instrumentation.

\subsection{Statistical Debugging}
\label{sec:debug}

Sampled information about program behavior can be thought of as a
database.  Data mining techniques, then, may reveal important
information about the aggregate behavior of many runs.  Of particular
interest to us is finding and fixing bugs.  We now illustrate how
mining of sampled program behavior can help direct software engineers
to the root cause of difficult bugs.

Because we do not have ready access to thousands of users, we simulate
a large user community by using randomly generated inputs in the
spirit of the Fuzz project~\cite{MKLMMNS95}.  Our selected target is
version 1.06 of the GNU implementation of \texttt{bc}.  We find that
feeding \texttt{bc} nine megabytes of random input causes it to crash
roughly one time in four.  \disregard{While this would be an unusually
  high failure rate for shipping software under normal usage patterns,
  we expect that users experiencing crashes will be more likely to
  opt-in to a trace reporting system.  Thus, this may be a reasonable
  ratio to expect if most crashes and a few non-crashes are reported.}

Quick perusal of the stack trace at a typical failure is discouraging.
The crash occurs several frames deep inside the C library's
\texttt{malloc()} function: a sure sign of heap corruption.  Such bugs
are especially pernicious because the actual corruption may have
happened well before the ultimate crash \cite{Eisenstadt1993b}.

We instrument \texttt{bc} to guess and randomly check a large number
of possible invariants.  Our goal is to identify predicates that hold
when the program succeeds, but which are violated when the program
crashes.  We cast an extremely broad net, but with an eye toward
pointer use errors and buffer overruns.  For pointer use, null
pointers are clearly of interest.  Relative addresses of pointers may
be interesting as well, as this may capture cases where one pointer
scans within a second pointed-to buffer.  Checking pointer/pointer
equality may reveal aliasing which, when not anticipated by the
programmer, can lead to dangling ``wild'' pointer bugs.  Scalar
variables serve as array indexes, pointer offsets, and in many other
roles; relationships among scalars may reveal buffer overruns,
unanticipated consequences of negative values, invalid enumeration
constants, or a variety of other problems.

At any direct assignment to a scalar variable $x$, we identify all
other local or global variables $\{ y_1, y_2, \dots, y_n \}$ which are
also in scope and which have the same type.  We then compare $x$ to
each $y_i$, and note whether $x$ was smaller, equal to, or greater
than $y_i$.  Even with very sparse sampling, a direct trace of these
comparisons would overwhelm both client, network, and server.
Instead, we locally reduce the trace by merely counting how often each
of the three relative orderings appeared.  We compare pointers to
same-typed pointers as well, and additionally compare each pointer for
equality with null.  One comparison between $x$ and $y_i$, which bumps
one of three counters, is considered to be one instrumentation site
subject to random sampling.  When an instrumented application
terminates, it emits the vector of counter triples along with a flag
indicating whether it completed successfully or was aborted by a fatal
signal.

As expected, this gives us a large number of candidate predicates:
13,442 counter triples, or 40,326 counters in all.  Because our
guesses are so crude, the vast majority of these are of no interest:
either they compare completely unrelated variables, or they express
relationships which behave indistinguishably in both successful and
failed runs.

To find the few predicates that do count, we recast the bug hunt as a
statistical analysis problem.  Each run of \texttt{bc} constitutes one
sample point consisting of 40,326 observed \termdef{features} and one
binary \termdef{outcome} ($0 = \text{succeeded}, 1 = \text{crashed}$).
In C, a misbehaving program can sometimes ``get lucky'': it may
violate memory safety yet not crash.  Hence the outcome label is
stochastic.  Given numerous data points (sampled runs), we would like
to identify a narrow subset of our 40,326 features which are relevant
to predicting the succeeded/crashed outcome.  This is the problem of
feature selection in a classification setting.

\termdef{Logistic regression} \cite{Hastie01} is a discriminative
binary classification method.  It is akin to linear regression, but
uses the logistic function instead of the linear output function.  Let
$x \in \Real^N$ be our input data, and $Y = \{0, 1\}$ be the output
label.  The output distribution is modeled as the logistic function:
%%
\begin{equation*}
  P(Y = 1 | X = x) = \frac{1}{ 1 + \exp(- \beta_0 - \vec\beta^T x) }
\end{equation*}

In our setting, the input data are the predicate counters, normalized
to have unit sample variance.  The $\beta$ values essentially weigh
the relative importance of each feature in the final outcome.  Since
we have many more parameters than data points, we constrain the
possible set of parameter values by penalizing large $\beta$ values
using the L1-norm, $\norm{\vec\beta}_1 = \sum_j \abs{\beta_j}$.  The
model is then trained using stochastic gradient ascent to reach a
local maximum of the penalized log likelihood.  Let $\mu(x) = P(Y = 1
| x)$.  Then:
%%
\begin{equation*}
  \begin{split}
    LL_\lambda (x, y, \{\beta_j\}) \:=\:
    & y \log \mu(x) + (1 - y) \log (1 - \mu(x)) \\
    & - \lambda \norm{\vec\beta}_1
  \end{split}
\end{equation*}

While other ways of doing feature selection exist, none of them are
particularly well-suited for this problem.  Some methods
\cite{Golub:MCC:1999,Tibshirani2002} calculate a univariate
correlation coefficient independently for each feature; other methods,
such as decision trees \cite{00000048}, are more computationally
intensive.  In our dataset, the features are clearly not independent
of each other, and the size of the problem is too large for more
computationally intensive methods.  Furthermore, logistic regression,
being a discriminative training method, does not require modeling the
distribution of the input features.  This is crucial, since our
features arise from a decidedly artificial process and would be
difficult to characterize using traditional statistical distributions.

Our \texttt{bc} dataset consists of 4058 runs with distinct random
inputs and distinct randomized 1/100 sampling.  We partition these
runs into 2704 to be used for training and 1354 for testing.  Although
there are 40,326 raw features, many can be discarded immediately: in
the training set 32,044 features are always zero and 1011 are always
some non-zero constant.  Hence the effective number of features used
in training is 7260.  Sparser sampling drives counts lower: at 1/1000,
the effective number of interesting features drops to 6738.  Beyond
simplifying the statistical analysis, this degree of regularity means
that counter reports are highly compressible, good news for deployed
users with small hard drives or slow modems.

In our experiments, the model converged after roughly four iterations
through the training dataset, using learning rate = $10^{-6}$ and
$\lambda = 5$.  This took fourteen minutes in MATLAB on a 1.8 GHz
Pentium 4 CPU with 1 GB of RAM.

Once the model has been trained, predicates with the largest $\beta$
coefficients suggest where to begin looking for the bug.  The top four
ranked predicates show a clear trend:

\newcounter{feature}
\begin{list}{\arabic{feature}.}{\usecounter{feature}\setlength{\itemsep}{0pt}\setlength{\parsep}{0in}\ttfamily\small}
\item storage.c:171: more\_arrays(): indx > break\_label
\item storage.c:171: more\_arrays(): indx < next\_func
\item storage.c:176: more\_arrays(): indx > optind
\item storage.c:171: more\_arrays(): indx > out\_col
\end{list}

The source code for \texttt{more\_arrays()} appears in
\autoref{fig:bc:more-arrays}.  An earlier comment suggests that this
one of a suite of ``three functions for increasing the number of
functions, variables, or arrays that are needed''.  The logic is a
fairly clear instance of the buffer reallocation idiom, even to one
unfamiliar with the code: line 167 allocates a larger chunk of memory;
line 171 is the top of a loop that transfers values over from the old,
smaller array; line 176 completes the resize by zeroing out the new
extra space.  As the comment suggests, there are two other similar
functions (\texttt{more\_functions()} and \texttt{more\_variables()})
nearby that do largely the same thing with different storage pools.
The text of these three functions is nearly identical, but each uses
different global variables (e.g. \texttt{a\_count} versus
\texttt{f\_count} versus \texttt{v\_count}).

Crashes are more likely when the loop index (\texttt{indx}) is smaller
than \texttt{next\_func} and larger than \texttt{break\_label},
\texttt{optind}, and \texttt{out\_col}.  None of these appear to have
a direct connection to \texttt{indx}, though: \texttt{optind} is part
of command line flag processing, \texttt{break\_label} concerns
evaluation of loops, \texttt{out\_col} helps output formatting, and
\texttt{next\_func} records how much storage has been consumed in the
pool managed by \texttt{more\_functions()}.

However, we must consider indirect effects as well.  If
\texttt{next\_func} is unusually large, it will tend to be larger than
\texttt{indx} even if the two are not strictly related.  Perhaps
crashes occur when \texttt{bc} tries to define too many functions.
Similarly, if \texttt{indx} is unusually large, then it will tend to
be larger then \texttt{break\_label}, \texttt{optind}, and
\texttt{out\_col}.  Perhaps crashes occur when the input to
\texttt{bc} defines large numbers of arrays.

As noted, \texttt{more\_arrays()} uses \texttt{indx} to transfer data
from an old storage buffer into a larger new one, and then to zero out
the extra space.  It may be worth examining these two loops' bounds
more closely, as a buffer overrun here could certainly trash the heap.
The allocation on line 167 requests space for \texttt{a\_count} items.
The copying loop on line 171 ranges from \texttt{1} through
\texttt{old\_count - 1}.  The zeroing loop on line 176 continues on
from \texttt{old\_count} through \texttt{v\_count - 1}.  And there we
find the bug: the new storage buffer has room for \texttt{a\_count}
elements, but the second loop is incorrectly bound by
\texttt{v\_count} instead.  After a quick glimpse at the neighboring
\texttt{more\_variables()} function it is clear that
\texttt{more\_arrays()} was created by copying and pasting
\texttt{more\_variables()} and then changing names like
\texttt{v\_count} and \texttt{v\_names} to \texttt{a\_count} and
\texttt{a\_names}.  The loop bound on line 176 was missed in the
renaming.

Having found the bug, it is reasonable to ask whether the statistical
analysis could have pointed at it more directly.  The mistaken use of
\texttt{v\_count} instead of \texttt{a\_count} on line 176 means that
a buffer overrun occurs when \texttt{indx > a\_count} on line 176.
This does correspond to a predicate sampled by our system, and this
predicate is the eleventh most highly ranked feature in the trained
model: significant, but still below six other features which do not
clearly relate to the bug.  Why was this, the smoking gun, not ranked
first?

There are several reasons to consider.  Samples are taken randomly,
while the model itself is trained using stochastic gradient ascent.
Thus, a degree of noise is fundamental to the process.  Even crashing
is not guaranteed: out of 320 runs in which sampling spotted
\texttt{indx > a\_count} at least once, 66 did not crash.  Thus, C
programs can ``get lucky'', meaning that this is not a strict
$\text{overrun} \implies \text{crash}$ implication.  Manual inspection
of the data reveals a high degree of redundancy among many
instrumentation sites within \texttt{more\_arrays()}, meaning that the
model has several features to choose from which have equivalent
predictive power.  This may suggest that our counters are too
fine-grained; that we are distinguishing many behaviors which are in
fact so tightly interrelated as to be equivalent.  Improving our
instrumentation scheme and fine-tuning the statistical analysis
methodology are key areas for continued development.

This bug seems clear enough once found.  However it has been present
and undiscovered at least since 1992 (the time\-stamp on this file in
the oldest version of GNU \texttt{bc} that we can find).  Many bugs
are obvious only once one knows where to look.  The logistic
regression results directed us to a fairly narrow range of less than a
dozen lines within a 33 line function, out of 8910 lines in
\texttt{bc} as a whole.  Our approach does not automatically find and
fix bugs.  But it does suggest where to start looking, and what sort
of scenarios (large \texttt{indx}) to consider.  Although we are still
learning about the capabilities of this system, and how to interpret
its results, we believe that statistically guided debugging has the
potential to make the process of finding and fixing bugs more
efficient.

\begin{figure}
  \centering
  \small
  \listinginput{152}{applications/more_arrays.c}
  \caption{Suspect \texttt{bc} function \texttt{more\_arrays()}}
  \label{fig:bc:more-arrays}
\end{figure}
